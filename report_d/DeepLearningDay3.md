```
【確認テスト】
サイズ5*5の入力画像をサイズ3*3のフィルタ、ストライド2でパディング1で畳み込んだ時の出力画像のサイズは？

3*3
```

---

# Section1：再起型ニューラルネットワークの概念
## 1.1 RNN
RNNとは時系列データに対応可能なニューラルネットワークである。

そもそも時系列データとは何か？　例えば音声データやテキストデータのことで、時間的順序を追って一定間隔ごとに観察され、相互に統計的依存関係が認められるようなデータの系列のことである。

RNNの特徴としては、時系列データを扱うために初期の状態と過去の時間t-iの状態を保持し、そこから次の時間tでの状態を再起的に求める再帰構造が必要となる。

```
【確認テスト】
RNNの3つの重みについてまとめる

1.入力層から中間層を定義する際にかけられる重み
2.中間層から出力層を定義する際にかけられる重み
3.中間層から中間層を定義する際にかけられる重み（過去の状態を次の時間以降の状態に反映させる）
```

```
【演習チャレンジ】
構文木を入力として再帰的に文全体の表現を得るにはどのように特徴を扱うか。

W.dot(np.concatenate([left, right]))のように、leftとrightの情報を損なわないように特徴を得る。
このままだとleftとrightを合わせた次元数に増えてしまうため、重み計算によって次元を戻したりする。
```

## 1.2 BPTT
BackPropagationThroughTimeの略、誤差を時間を遡って逆伝播させるRNNにおけるパラメータ調整方法の一つである。

```
【確認テスト】
連鎖律の原理を用いて、dz/dxを求めよ。

z=t^2, t=x+y

dz/dx = dz/dt * dt/dx
      = 2t * 1
      = 2(x+y)
```

```
【確認テスト】
図のy_1をx, s_0, s_1, w, w_in, w_outを用いて表せ。

y_1 = g(w_out・s_1 + c)
    = g(w_out・f(w_in・x_1 + w・s_0 + b) + c)
```

ソースコードの解読が難しかったので、3桁の加算機を例に計算グラフを書いて確認。（下記は概要で、δ[1]ぐらいまで遡っていってようやく理解できました）
* δ[2] = （d_out[2]とW_outの内積） * d_sig
* W_out_grad = d_out[1] * z[1]
* δ[1] = (δ[2]とWの内積 + d_z[1]) * d_sig

```
【コード演習問題】
bptt_stepのfor文中のdelta_tの更新式は？

W,U,VをW_in,W,W_outで書き直すとそれぞれ下記の通り
・W:W_in
・U：W
・V：W_out

delta_tは次のループに持ち越されて計算するので、bptt_stepのfor文内はそこまで算出していたdeltaとWとの内積になる。
よってdelta_t=delta_t.dot(U)になる
```

## 実装
演習を実行したgoogle colaboratoryのURL：https://colab.research.google.com/drive/1Muytok5Ymao_fohmZ-BfXQG-FzNuCtS2?usp=sharing

確認した点：
* 学習が進んでいない段階では計算結果はほぼ間違っているが、学習が進んだ段階では計算結果がほぼ正解していること
* np.unpackbitsの結果
* 順伝播での再帰箇所確認
* 逆伝播の勾配計算時は桁数の上の方から下降していくこと
* delta[:,8]は更新されないので0のままであること

# Section2：LSTM
## RNNの課題
時系列を遡れば遡るほど、勾配が消失していくので長い時系列の学習が困難である。

```
【確認テスト】
シグモイド関数を微分した時、入力値が0のときに最大値をとる。その値は？

0.25
```

勾配爆発とは、層を逆伝播するごとに勾配が指数関数的に大きくなっていくこと。

RNNでは勾配消失や勾配爆発が起こる傾向がある。

```python
# 【演習チャレンジ】
# クリッピング関数の戻り値は？

norm = np.linalg.norm(grad)
rate = threshold / norm
if rate < 1:
  return grad * rate
return grad
```

## 2-1.CEC
勾配消失および勾配爆発の解決策として、勾配が1であれば解決できる。
ただし単体で使うことを考えると、入力データについて時間依存度に関係なく重みが一律になってしまうため、別の要素を用意してニューラルネットワークを学習させる。

## 2-2.入力ゲートと出力ゲート
入出力ゲートの役割は、それぞれのゲートへの入力値（現在の入力データと1つ前の出力データ）の重みを重み行列で可変にし、CECの課題を解決すること。

## 2-3.忘却ゲート
CECは過去の情報が全て保管されており、不要な過去の情報を削除することができないのが課題である。そこで、過去の情報が不要になったタイミングで情報を忘却できる機能としてこのゲートを用意する。

```
【確認テスト】
LSTMである文章内の空欄の文字を予測する際、無くなっても予測に影響がなさそうな文字に対してはどのゲートが作用するか。

忘却ゲート
```
```python
# 【演習チャレンジ】
# LSTMのCECの更新式を書く

a = np.tanh(a)
input_gate = _sigmoid(i)
forget_gate = _sigmoid(f)
output_gate = _sigmoid(o)

c = input_gate * a + forget_gate * c
h = output_gate * np.tanh(c)
```

## 2-4.覗き穴結合
CEC自身の値はゲート制御に影響を与えていないので、CECに保存されている過去の情報を使ってゲート制御できるようにしたい。

CEC自身の値に重み行列を介して伝播可能にした構造を覗き穴結合という。ただしそんなに精度は向上しなかった様子（計算量は増える）。

# Section3：GRU
LSTMではパラメータ数が多く、計算負荷が高くなる問題があった。しかしGRUではそのパラメータを大幅に削減して計算負荷を下げ、精度は同等かデータによってはそれ以上の精度を望めるようにした。

GRUはGated Recurrent Unitの略で、ゲート付き回帰型ユニットとも呼ぶ。

```
【確認テスト】
LSTMとCECが抱える課題は？

CECはRNNの課題を解決するために導入されたが勾配が1なのでそのままではニューラルネットワークが学習できない。
LSTMはCECに加えて入力、出力、忘却ゲートを導入することでニューラルネットワークを学習できるようにしたが、パラメータが増えたせいで計算量が増えてしまった。
```

```python
# 【演習チャレンジ】
# GRUの出力部分のソースコードを書く

x = <現在の入力>
h = <１つ前の出力>

r = _sigmoid(x.dot(W_r.T) + h.dot(U_r.T)
z = _sigmoid(x.dot(W_z.T) + h.dot(U_z.T)

h_bar = np.tanh(x.dot(W.T) + (r * h).dot(U.T))
h_new = (1 - z) * h + z * h_bar
```
文献によっては1ステップ前の中間層の状態を (1 - z) でかけたり z でかけたりしているが、本質的に同じなのかどうか。（資料とチャンレンジ問題のソースでも逆になっている）

```
【確認テスト】
LSTMとGRUの違い

LSTMはCECがあり、入力ゲート、出力ゲート、忘却ゲートを持つ
GRUはCECがなく、リセットゲートと更新ゲートのみを持つ
GRUの方が計算量が少なく、精度はほぼ変わらない
```

## 実装
演習を実行したgoogle colaboratoryのURL：https://colab.research.google.com/drive/1J7h2ss7KsmdRHs4W-hZuMCfg_UZP_z7f?usp=sharing

確認した点：
* tensorflowのライブラリを利用する方法を確認
* 実際に学習させようとすると時間がかかるので、学習済みの結果を利用して識別結果を確認

# Section4：双方向RNN
過去の情報だけでなく未来の情報を加味することで、精度を向上させるためのモデル。機械翻訳など、未来の文章データも学習に使える事例で実用できる。BidirectionalRNNと呼ぶこともある。

RNNでは過去から未来の方向のみ伝播だったが、双方向RNNでは過去から未来の方向に加え、未来から過去の方向にも伝播させる。

```python
# 【演習チャレンジ】
# 双方向RNNの順伝播で、ある時間で2つの中間層（過去から未来、未来から過去）の出力をどのように扱って出力層に与えるか

hs_f = _rnn(xs_f, W_f, U_f)
hs_b = _rnn(xs_b, W_b, U_b)
hs = [np.concatenate([h_f, h_b[::-1]], axis=1) for h_f, h_b in zip(hs_f, hs_b)]
ys = hs.dot(V.T)
```
zipによってhs_fとhs_bの要素をそれぞれ1つずつ取り出し、[h_f, h_b[::-1]]という配列を作り、ループごとにconcatenateのaxis=1方向に連結させる。

# Section5：Seq2Seq
Seq2SeqはEncorder-Decorderモデルの一種で、機械対話や機械翻訳などに使用される。例題で用いた加算器も、計算式を入力して計算結果を出力するので、計算式（seq）から計算結果（seq）へ変換していると捉えることもできそう。
## 5-1.Encorder RNN
* Taking :文章を単語等のトークン毎に分割し、トークンごとのIDに分割する。
* Embedding :IDから、そのトークンを表す分散表現ベクトルに変換する。
* Encoder RNN:ベクトルを順番にRNNに入力していく。

処理手順
1. vec1をRNNに入力し、hidden stateを出力する。このhidden stateと次の入力vec2をまたRNNに入力し、hidden stateを出力するという流れを繰り返す。
2. 最後のvecを入れたときのhidden stateをfinal stateとしてとっておく。このfinal stateがthought vectorと呼ばれ、**入力した文の意味を表すベクトル**となる。

## 5-2.Decorder RNN
処理手順
1. Decoder RNN: Encoder RNN のfinal state (thought vector) から、各token の生成確率を出力していく。final state をDecoder RNN のinitial state として設定し、Embedding を入力。
2. Sampling:生成確率にもとづいて token をランダムに選ぶ。
3. Embedding:2で選ばれた token を Embedding して Decoder RNN への次の入力とする。
4. Detokenize:1〜3 を繰り返し、2で得られたtoken を文字列に直す。

```
【確認テスト】
seq2seqについて説明している文はどれか。

(2)RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。
```
```python
# 【演習チャレンジ】
# 入力である文（文章）を時系列の情報をもつ特徴量へとエンコードする関数について書く。

# words: sequence words, one-hot vector, (n_words, vocab_size)
# E: word embeding matrix, (embed_size, vocab_size)

for w in words:
  e = E.dot(w)
  h = _activation(W.dot(e) + U.dot(h) + b)
```
dot積の計算は基本的に行列のshapeを意識しないといけないが、対象がベクトルの場合は適当に変換してくれる。w:(1,vocab_size) -> (vocab_size,1)
ただし、上記の例でEのshapeが誤っていると計算ができない。（E.T.dot(w）だと(vocab_size, embed_size)*?になるので、shapeからも計算ができないことがわかる。

## 5-3.HRED
seq2seq2の課題は、事前の文脈を考慮できず、一問一答しかできないことである。

HREDでは過去の発話から次の会話を生成することができるので、より人間らしい文章が生成される。構造的には seq2se2 + Context RNN 。
ただし、汎用的な文章を生成しがち（どんな会話でも同じような内容を発言しがち）な課題がある。

## 5-4.VHRED
VHREDは、HREDに VAEの潜在変数の概念を追加することで解決した構造

```
【確認テスト】
seq2seq2とHRED、HREDとVHREDの違いを述べよ。

seq2seq：一つの時系列データから別の時系列データを得るネットワークである。ただし一問一答である。
HRED：文脈の意図を汲み取り文章を生成できる構造である。
VHRED：VAEの概念を追加し、HREDでは汎用的な文章を生成しがちだった課題を解決するよう改良された構造である。
```

## 5-5.VAE
### オートエンコーダ
教師なし学習の一つで、Encorder部とDecorder部から構成される。

* Encorder：入力データから潜在変数zに変換するニューラルネットワーク
* Decorder：潜在変数zを入力として元画像（データ）を復元するニューラルネットワーク

入力データより潜在変数zの次元を小さくできた場合、次元削減ができることになる。

### VAE
オートエンコーダーでは潜在変数zの構造を解読しにくい。そこで潜在変数zに確率分布z~N(0,1)を仮定することにより、解読しやすくしたものをVAEと呼ぶ。
ただし元のオートエンコーダーの出力画像に比べると、若干ボヤけたような画像になってしまう課題があった。

```
【確認テスト】
次の文章を完成させる。

VAEは自己符号化器の潜在変数に確率分布を導入したもの。
```

# Section6：Word2vec
RNNの課題「可変長の文字列をニューラルネットワークに与えることができない」を解決でき、そのためには固定長形式で可変長の文字列を表す必要がある。

学習データからボキャブラリ（単語の種類分）を生成し、対応する単語をone-hotベクトルに置き換える。

メリットとしては、大規模データの分散表現の学習が、現実的な計算速度とメモリ量で実現可能にした。

またone-hotベクトル表現のままではスパースなので、(ボキャブラリ数, 単語ベクトル)次元の重み行列によってさらに次元数を落としたり、単語同士の関連性を考慮できるようにする（前述のEmbedding）。

# Section7：AttentionMechanism
seq2seqの課題「長い文章への対応」を解決できる。

解決策
1. まずは過去時間のデータを全て用いた行列表現を内部表現に用いる。
2. ただしこれだけだと文章が長くなるほどシーケンスの内部表現の次元も大きくなっていくので、「入力と出力のどの単語が関連しているか」の関連度を学習する。

```
【確認テスト】
RNNをword2vec、seq2seq2とAttentionの違いを述べよ。

RNN：時系列データを処理するのに適したネットワークである。
word2vec：単語の分散ベクトルを得る手法である。
seq2seq：一つの時系列データから別の時系列データを得るネットワークである。
Attention：時系列データの中身の関連性に対して重みをつける手法である。
```
