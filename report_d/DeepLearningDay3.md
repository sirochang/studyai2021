```
【確認テスト】
サイズ5*5の入力画像をサイズ3*3のフィルタ、ストライド2でパディング1で畳み込んだ時の出力画像のサイズは？

3*3
```

---

# Section1：再起型ニューラルネットワークの概念
## 1.1 RNN
RNNとは時系列データに対応可能なニューラルネットワークである。

そもそも時系列データとは何か？　例えば音声データやテキストデータのことで、時間的順序を追って一定間隔ごとに観察され、相互に統計的依存関係が認められるようなデータの系列のことである。

RNNの特徴としては、時系列データを扱うために初期の状態と過去の時間t-iの状態を保持し、そこから次の時間tでの状態を再起的に求める再帰構造が必要となる。

```
【確認テスト】
RNNの3つの重みについてまとめる

1.入力層から中間層を定義する際にかけられる重み
2.中間層から出力層を定義する際にかけられる重み
3.中間層から中間層を定義する際にかけられる重み（過去の状態を次の時間以降の状態に反映させる）
```

```
【演習チャレンジ】
構文木を入力として再帰的に文全体の表現を得るにはどのように特徴を扱うか。

W.dot(np.concatenate([left, right]))のように、leftとrightの情報を損なわないように特徴を得る。
このままだとleftとrightを合わせた次元数に増えてしまうため、重み計算によって次元を戻したりする。
```

## 1.2 BPTT
BackPropagationThroughTimeの略、誤差を時間を遡って逆伝播させるRNNにおけるパラメータ調整方法の一つである。

```
【確認テスト】
連鎖律の原理を用いて、dz/dxを求めよ。

z=t^2, t=x+y

dz/dx = dz/dt * dt/dx
      = 2t * 1
      = 2(x+y)
```

```
【確認テスト】
図のy_1をx, s_0, s_1, w, w_in, w_outを用いて表せ。

y_1 = g(w_out・s_1 + c)
    = g(w_out・f(w_in・x_1 + w・s_0 + b) + c)
```

ソースコードの解読が難しかったので、3桁の加算機を例に計算グラフを書いて確認。（下記は概要で、δ[1]ぐらいまで遡っていってようやく理解できました）
* δ[2] = （d_out[2]とW_outの内積） * d_sig
* W_out_grad = d_out[1] * z[1]
* δ[1] = (δ[2]とWの内積 + d_z[1]) * d_sig

```
【コード演習問題】
bptt_stepのfor文中のdelta_tの更新式は？

W,U,VをW_in,W,W_outで書き直すとそれぞれ下記の通り
・W:W_in
・U：W
・V：W_out

delta_tは次のループに持ち越されて計算するので、bptt_stepのfor文内はそこまで算出していたdeltaとWとの内積になる。
よってdelta_t=delta_t.dot(U)になる
```

## 実装
演習を実行したgoogle colaboratoryのURL：https://colab.research.google.com/drive/1Muytok5Ymao_fohmZ-BfXQG-FzNuCtS2?usp=sharing

確認した点：
* 学習が進んでいない段階では計算結果はほぼ間違っているが、学習が進んだ段階では計算結果がほぼ正解していること
* np.unpackbitsの結果
* 順伝播での再帰箇所確認
* 逆伝播の勾配計算時は桁数の上の方から下降していくこと
* delta[:,8]は更新されないので0のままであること

# Section2：LSTM
## RNNの課題
時系列を遡れば遡るほど、勾配が消失していくので長い時系列の学習が困難である。

```
【確認テスト】
シグモイド関数を微分した時、入力値が0のときに最大値をとる。その値は？

0.25
```

勾配爆発とは、層を逆伝播するごとに勾配が指数関数的に大きくなっていくこと。

RNNでは勾配消失や勾配爆発が起こる傾向がある。

```python
# 【演習チャレンジ】
# クリッピング関数の戻り値は？

norm = np.linalg.norm(grad)
rate = threshold / norm
if rate < 1:
  return grad * rate
return grad
```

## 2-1.CEC
勾配消失および勾配爆発の解決策として、勾配が1であれば解決できる。
ただし単体で使うことを考えると、入力データについて時間依存度に関係なく重みが一律になってしまうため、別の要素を用意してニューラルネットワークを学習させる。

## 2-2.入力ゲートと出力ゲート
入出力ゲートの役割は、それぞれのゲートへの入力値（現在の入力データと1つ前の出力データ）の重みを重み行列で可変にし、CECの課題を解決すること。

## 2-3.忘却ゲート
CECは過去の情報が全て保管されており、不要な過去の情報を削除することができないのが課題である。そこで、過去の情報が不要になったタイミングで情報を忘却できる機能としてこのゲートを用意する。

```
【確認テスト】
LSTMである文章内の空欄の文字を予測する際、無くなっても予測に影響がなさそうな文字に対してはどのゲートが作用するか。

忘却ゲート
```
```python
# 【演習チャレンジ】
# LSTMのCECの更新式を書く

a = np.tanh(a)
input_gate = _sigmoid(i)
forget_gate = _sigmoid(f)
output_gate = _sigmoid(o)

c = input_gate * a + forget_gate * c
h = output_gate * np.tanh(c)
```

## 2-4.覗き穴結合
CEC自身の値はゲート制御に影響を与えていないので、CECに保存されている過去の情報を使ってゲート制御できるようにしたい。

CEC自身の値に重み行列を介して伝播可能にした構造を覗き穴結合という。ただしそんなに精度は向上しなかった様子（計算量は増える）。

# Section3：GRU
LSTMではパラメータ数が多く、計算負荷が高くなる問題があった。しかしGRUではそのパラメータを大幅に削減して計算負荷を下げ、精度は同等かデータによってはそれ以上の精度を望めるようにした。

GRUはGated Recurrent Unitの略で、ゲート付き回帰型ユニットとも呼ぶ。

```
【確認テスト】
LSTMとCECが抱える課題は？

CECはRNNの課題を解決するために導入されたが勾配が1なのでそのままではニューラルネットワークが学習できない。
LSTMはCECに加えて入力、出力、忘却ゲートを導入することでニューラルネットワークを学習できるようにしたが、パラメータが増えたせいで計算量が増えてしまった。
```

```python
# 【演習チャレンジ】
# GRUの出力部分のソースコードを書く

x = <現在の入力>
h = <１つ前の出力>

r = _sigmoid(x.dot(W_r.T) + h.dot(U_r.T)
z = _sigmoid(x.dot(W_z.T) + h.dot(U_z.T)

h_bar = np.tanh(x.dot(W.T) + (r * h).dot(U.T))
h_new = (1 - z) * h + z * h_bar
```
文献によっては1ステップ前の中間層の状態を (1 - z) でかけたり z でかけたりしているが、本質的に同じなのかどうか。（資料とチャンレンジ問題のソースでも逆になっている）

```
【確認テスト】
LSTMとGRUの違い

LSTMはCECがあり、入力ゲート、出力ゲート、忘却ゲートを持つ
GRUはCECがなく、リセットゲートと更新ゲートのみを持つ
GRUの方が計算量が少なく、精度はほぼ変わらない
```

# Section4：双方向RNN
過去の情報だけでなく未来の情報を加味することで、精度を向上させるためのモデル。機械翻訳など、未来の文章データも学習に使える事例で実用できる。BidirectionalRNNと呼ぶこともある。

RNNでは過去から未来の方向のみ伝播だったが、双方向RNNでは過去から未来の方向に加え、未来から過去の方向にも伝播させる。

```python
# 【演習チャレンジ】
# 双方向RNNの順伝播で、ある時間で2つの中間層（過去から未来、未来から過去）の出力をどのように扱って出力層に与えるか

hs_f = _rnn(xs_f, W_f, U_f)
hs_b = _rnn(xs_b, W_b, U_b)
hs = [np.concatenate([h_f, h_b[::-1]], axis=1) for h_f, h_b in zip(hs_f, hs_b)]
ys = hs.dot(V.T)
```
zipによってhs_fとhs_bの要素をそれぞれ1つずつ取り出し、[h_f, h_b[::-1]]という配列を作り、ループごとにconcatenateのaxis=1方向に連結させる。

# Section5：Seq2Seq
# Section6：Word2vec
# Section7：AttentionMechanism
