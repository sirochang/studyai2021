# Section1:強化学習
## 強化学習とは
長期的に「報酬」を最大化できるように「環境」の中で「行動」を選択できる「エージェント」を作ることを目標とする機械学習の一分野である。

## 強化学習の応用例
マーケティングの例

* 環境： 会社の販売促進部
* エージェント：プロフィールと購入履歴に基づいて、キャンペーンメールを送る顧客を決めるソフトウェア
  * 行動：顧客ごとに送信・非送信の二つの行動を選ぶ
  * 報酬：負の報酬（キャンペーンのコスト）と正の報酬（キャンペーンで生み出されると推測される売り上げ）を受ける

## 探索と利用のトレードオフ
環境において事前に完璧な知識があれば、最適な行動を予測し決定することは可能だが、強化学習の場合はこれが成り立たず、不完全な知識を元に行動しながらデータを収集し、最適な行動を見つけていくことになる。

下記はトレードオフの関係にある。
* 探索が足りない状態
  * 過去のデータでベストとされる行動のみを常に取り続ければ、他にもっとベストな行動があったとしても見つけれない
* 利用が足りない状態
  * 未知の行動のみを常に取り続ければ、過去の経験が活かせない

## 強化学習の差分
教師あり・なし学習との差分は目標が違うことが挙げられる。
* 教師あり・なし学習
  * データに含まれるパターンを見つけ出す、そのデータから未知のデータを予測する
* 強化学習
  * 優れた方策を見つけ出す

研究は昔から行われていたが、計算速度の観点からあまり実用化されなかった（教師あり・なし学習より計算コストがかかりがち）。近年は計算速度の進展や、下記の手法を組み合わせる手法により実用化が進んでいる。

* Q学習
  * 行動価値関数を、行動をするごとに更新することにより学習を進める
* 関数近似法
  * 価値関数や方策関数を関数近似する（ニューラルネットワークを利用できる）

## 行動価値関数
下記の2つ。

* 状態価値関数
  * ある状態の価値に注目する
* 行動価値関数
  * ある状態である行動を取った時の価値に注目する

## 方策関数
方策ベースの強化学習において、ある状態でどのような行動を採るのかの確率を与える関数である。

## 方策勾配法
方策反復法：方策をモデル化して最適化する手法→方策勾配法。基本的な形は勾配降下法の更新式に似ているが、損失の勾配を減算していたのに対し、Jを加算していることに注意

```
θ^(t+1) = θ^t + ε∇J(θ)
```

Jは収益を表すもので、平均報酬や割引報酬和の定義に対して行動価値関数を定義し、方策勾配定理が成り立つので ∇J(θ)を導き出せるらしい。

# Section2:AlphaGo
# Section3:軽量化・高速化技術
# Section4:応用モデル
# Section5:Transformer
# Section6:物体検知・セグメンテーション
