# 概要
「応用数学」について章ごとに要点をまとめる。

## 第1章 線形代数
### 1.1 行列の基本
* スカラー：普通の数で、「大きさ」を表す
* ベクトル：「大きさ」と「向き」を表す
* 行列：ベクトルのベクトル

この章では主に、行列をベクトルの変換に用いることを考える。
行列の「積」は特殊な計算だが、連立1次方程式を解く上でよりシンプルに書き表すことができる。
* 通常の解き方：行基本変形を用いる
* 行列を用いた解き方：行基本変形に相当する行列の変形で解くことができる
  * **逆行列**（行列の逆数のような存在）を両辺に乗じることにより、方程式を解くようにして変数の行列の解を導出できる

逆行列の算出法は**掃き出し法**や**余因子行列を用いる方法**などがある。

行列を用いた連立1次方程式の解き方は**掃き出し法**や**クラメルの公式**を用いる方法がある。

行列式は逆行列の計算や存在確認、固有値や特異値の算出に用いることができる。
* 3次の場合は、**サラスの公式**で簡単に解くことができる

### 1.2 固有値分解
固有値と固有ベクトルはそれぞれスカラーとベクトルであり、ある行列Aに対して行列Aと固有ベクトルの積が固有値と固有ベクトルの積と等しくなる。
さらに、行列Aは固有値を対角線に並べた行列Λと固有ベクトルを並べた行列 V およびその逆行列V^{-1}を用いて表すことができる。
```
A = VΛV^{-1}
```
この形式で表すことにより、**行列Aの累乗計算を容易に行うことができる**ようになる。
```
A^n = (VΛV^{-1})^n = VΛV^{-1}VΛV^{-1}...(n回)...VΛV^{-1} = VΛ^nV^{-1}
```

### 1.3 特異値分解
固有値分解は正方行列でしかできないが、似たような方法で行列を分解することができる。

特異値分解は画像認識の分野で**次元圧縮**や**主成分分析**などに用いられたりする。

### 1.4 参考文献
* マセマ出版社 「線形代数 キャンパス・ゼミ 改訂2」

---
## 第2章 確率・統計
### 2.1 集合
* 集合：ある一定の条件を満たすものの集まり
* 2つの集合A, Bについて
  * 和集合：A∪Bと表し「AカップB」と呼ぶ、AまたはBのいずれかに属する要素全体の集合
  * 共通部分：A∩Bと表し「AキャップB」と呼ぶ、AとBに共通な要素全体の集合
* 全体集合：考えている対象の全てを要素とする集合
* 補集合：
  * 絶対補：U\Aと表し（もしくはAバー）、全体集合Uに属するがAには属さない要素の集合
  * 相対補：B\Aと表し、集合Bに属するがAには属さない要素の集合
* 空集合：1つの要素も持っていない集合

### 2.2 条件付き確率と同時確率
* 条件付き確率：ある事象X=xが与えられた条件で、Y=yとなる確率
* 同時確率：ある事象X=xとY=yが同時に起きる確率
* 独立な事象の同時確率：ある事象X=xとY=yがそれぞれ因果関係が無ければ、同時確率はそれぞれの事象が起きる確率の掛け算になる

### 2.3 ベイズ則
同時確率を条件付き確率で表してみる。
```
P(X=x, Y=y) = P(X=x| Y=y) * P(Y=y)
P(Y=y, X=x) = P(Y=y| X=x) * P(X=x)

P(X=x, Y=y) = P(Y=y, X=x)なので

P(X=x| Y=y) * P(Y=y) = P(X=x, Y=y)
                     = P(Y=y, X=x)
                     = P(Y=y| X=x) * P(X=x)
```

### 2.4 期待値と分散
* 確率変数：事象と結びつけられた**数値**
* 確率分布：事象の発生する**確率の分布**
* 期待値：その分布における確率変数の平均値
* 分散：データの散らばり具合
* 共分散：2つのデータ系列の傾向の違い
* 標準偏差：分散は2乗しているので元のデータと単位が異なるため、平方根を取って元のデータと単位を揃えている

### 2.5 確率分布
* ベルヌーイ分布：足すと1になる2つの確率が発生する確率分布
* マルチヌーい分布：足すと1になる3つ以上の確率が発生する確率分布
* 二項分布：ベルヌーイ分布の多施行版
* ガウス分布：釣鐘型の連続分布

### 2.6 推定
* 母集団：対象としている集合の全要素から得られる数値全体のデータの集まり
* 推定：母集団を特徴付ける母数（パラメーター）を統計学的に推測すること
  * 点推定：平均値などを1つの値に推定すること
  * 区間推定：平均値などが存在する範囲を推定すること

* 標本平均：母集団から取り出した標本の平均で、**一致性と不偏性を満たす**
* 標本分散：母集団から取り出した標本の分散で、**一致性を満たすが不偏性を満たさない**
* 不偏分散：不偏性も満たすように標本分散にn/(n-1)をかける（平均を用いて表すことにより自由度がn-1なので、n-1で割るようにしている）

### 2.7 参考文献
* マセマ出版社 「統計学 キャンパス・ゼミ」

---
## 第3章 情報理論
### 3.1 自己情報量
変化した数値が同じでも、全体（w）に対する変化量（Δw）はケースによって異なる。
連続した量の足し合わせを考えると1/wの積分で表すことができる。（積分を計算するとlog（w）となる）

xの関数として考えると、自己情報量I(x)は下記のように表すことができる。
```
I(x) = log(W(x))
```

確率P(x)を用いると、W(x)の逆数になるので下記のように表すこともできる。
```
I(x) = - log(P(x))
```
* 単位
  * bit（ビット）：対数の底が2
  * nat（ナット）：対数の底が自然数

### 3.2 シャノンエントロピー
自己情報量の期待値をシャノンエントロピーH(x)と表す。
```
H(x) = E(I(x))
     = - E (log(P(x))）
     = - Σ (P(x) * log(P(x)))
```

### 3.3 カルバック・ライブラー・ダイバージェンス
同じ事象・確率変数における異なる確率分布P,Qの違いを表す。

**確率分布Pに注目したとき、まず情報量の差**は下記で表せる。
```
(情報量の差) = I(Q(x)) - I(P(x))
           = log(P(x)/Q(x))
```

上記で求めた**情報量の差に対して確率分布Pについて期待値を計算**する。
```
DKL(P||Q) = Σ (P(x) * (情報量の差）)
          = Σ (P(x) * log(P(x)/Q(x)))
```

### 3.4 交差エントロピー
KLダイバージェンスの一部分を取り出したもので、**Qについての自己情報量をPの分布で平均したもの**をH(P,Q)と表す。
```
H(P,Q) = Σ (P(x) * (Qの自己情報量))
       = Σ (P(x) * (- log(Q(x))))
       = - Σ (P(x) * log(Q(x)))
```
ニューラルネットワークでモデルを学習する際に、指標を表す**損失関数**として交差エントロピーの考え方が使われることもある。

### 3.5 参考文献
* O'REILLY 「ゼロから作るDeep Learning」
