# プロローグ
## 識別と生成
### 識別
データを対象のクラスに分類するのが目的で、条件「あるデータが与えられた」という条件のもとで対象クラスである確率を求める。
### 生成
対象のクラスのデータを生成するのが目的で、条件「あるクラスに属する」という条件のもとでのデータの分布を求める。

## 識別モデルと生成モデル
### 識別モデル
高次元のデータから低次元のデータへ変換するモデルで、生成モデルと比べると学習に必要なデータ量が少ない。画像認識等に応用される。

### 生成モデル
低次元のデータから高次元のデータへ変換するモデルで、識別モデルと比べると学習に必要なデータ量が多い。画像の超解像やテキスト生成等に応用される。

---

# ニューラルネットワークの全体像

```
【確認テスト】
ディープラーニングとは、明示的なプログラムの代わりに多数の中間層を持つニューラルネットワーク（NN）を用いて、入力値から目的とする出力層に変換する数学モデルを構築することである。
各層の入力部分である重みとバイアスを最適化することが目的である。
```

```
【確認テスト】
次のNNを図示する。
　・入力層：2ノード1層
　・中間層：3ノード2層
　・出力層：1ノード1層
TODO
```

NNは回帰と分類のどちらの問題も解くここができる
* 回帰：連続する実数値を取る関数の近似
  * 線形回帰
  * 回帰木
  * ランダムフォレスト
  * NN
* 分類：離散的な結果を予想するための分析
  * ベイズ分類
  * ロジスティック回帰
  * 決定木
  * ランダムフォレスト
  * NN

---

# Section1：入力層〜中間層
入力とバイアスと層間の重みの線形結合を活性化関数で変換して得られた結果を結合している層の出力とする（次の層の入力となる）。

```python
# 【確認テスト】
# u = Wx + b をpythonのコードで表す。

import numpy as np

# 重み
W = np.array([[0.1], [0.2]])

# バイアス
b = 0.5

# 入力値
x = np.array([2, 3])

# 総入力
u = np.dot(x, W) + b
```

スライドNo17の図とサンプルコードが一致していないように思われます。
* 入力層として、xを活性化関数で変換してz(1)を出力としている？
* 図を見るとz(1)とW(1)とb(1)の線形結合がu(1)となっているが、ソースではu(1)を活性化関数で変換した出力をz(1)としている

以上を踏まえ、下記のように解釈しました。
```python
# 【確認テスト】
# スライドNo17のNNの中間層の出力を定義しているソースコードは？

# 入力層から中間層への重みとバイアス
W1, b1 = network['W1'], network['b1']

# 中間層から出力層への重みとバイアス
W2, b2 = network['W2'], network['b2']

# 中間層への入力u1と出力z1
u1 = np.dot(x, W1) + b1
z1 = functions.relu(u1)

# 出力層への入力u2と出力z2
u2 = np.dot(z1, W2) + b2
z2 = functions.relu(u2)
```

# Section２：活性化関数
NNにおいて、次の層への出力の大きさを決める**非線形関数**であり、入力値によって次の層への信号のON/OFFや強弱を定める働きを持つ。
```
# 【確認テスト】
線形と非線形の違い

線形な関数は
・加法性：f(x+y)=f(x)+f(y)
・斉次性：f(kx)=kf(x)
を満たす。

非線形な関数は加法性・斉次性を満たさない。
```
中間層と出力層でそれぞれ用いられる活性化関数がある。
* 中間層
  * ステップ関数：閾値を超えたら発火する関数で出力は常に1か0。線形分離可能なものしか学習できなかった。
  * シグモイド関数：0~1の間を緩やかに変化する関数で、信号の強弱を伝えられるようになり予想NNの普及のきっかけとなった。大きな値では出力の変化が微小なため、勾配消失問題が起きることがあった。
  * ReLU関数：ある値を境に、0か入力値を取る関数。勾配消失問題の回避とスパース化に貢献する。
* 出力層
  * ソフトマックス関数
  * 恒等写像
  * シグモイド関数

```python
# 【確認テスト】
# 活性化関数をソースコードから抜き出す。functions.relu()の部分が該当箇所

z1 = functions.relu(u1)
z2 = functions.relu(u2)
```

# Section3：出力層
# Section4：勾配降下法
# Section5：誤差逆伝搬法
