# プロローグ
## 識別と生成
### 識別
データを対象のクラスに分類するのが目的で、条件「あるデータが与えられた」という条件のもとで対象クラスである確率を求める。
### 生成
対象のクラスのデータを生成するのが目的で、条件「あるクラスに属する」という条件のもとでのデータの分布を求める。

## 識別モデルと生成モデル
### 識別モデル
高次元のデータから低次元のデータへ変換するモデルで、生成モデルと比べると学習に必要なデータ量が少ない。画像認識等に応用される。

### 生成モデル
低次元のデータから高次元のデータへ変換するモデルで、識別モデルと比べると学習に必要なデータ量が多い。画像の超解像やテキスト生成等に応用される。

---

# ニューラルネットワークの全体像

```
【確認テスト】
ディープラーニングとは、明示的なプログラムの代わりに多数の中間層を持つニューラルネットワーク（NN）を用いて、入力値から目的とする出力層に変換する数学モデルを構築することである。
各層の入力部分である重みとバイアスを最適化することが目的である。
```

```
【確認テスト】
次のNNを図示する。
　・入力層：2ノード1層
　・中間層：3ノード2層
　・出力層：1ノード1層
TODO
```

NNは回帰と分類のどちらの問題も解くここができる
* 回帰：連続する実数値を取る関数の近似
  * 線形回帰
  * 回帰木
  * ランダムフォレスト
  * NN
* 分類：離散的な結果を予想するための分析
  * ベイズ分類
  * ロジスティック回帰
  * 決定木
  * ランダムフォレスト
  * NN

---

# Section1：入力層〜中間層
入力とバイアスと層間の重みの線形結合を活性化関数で変換して得られた結果を結合している層の出力とする（次の層の入力となる）。

```python
# 【確認テスト】
# u = Wx + b をpythonのコードで表す。

import numpy as np

# 重み
W = np.array([[0.1], [0.2]])

# バイアス
b = 0.5

# 入力値
x = np.array([2, 3])

# 総入力
u = np.dot(x, W) + b
```

スライドNo17の図とサンプルコードが一致していないように思われます。
* 入力層として、xを活性化関数で変換してz(1)を出力としている？
* 図を見るとz(1)とW(1)とb(1)の線形結合がu(1)となっているが、ソースではu(1)を活性化関数で変換した出力をz(1)としている

以上を踏まえ、下記のように解釈しました。
```python
# 【確認テスト】
# スライドNo17のNNの中間層の出力を定義しているソースコードは？

# 入力層から中間層への重みとバイアス
W1, b1 = network['W1'], network['b1']

# 中間層から出力層への重みとバイアス
W2, b2 = network['W2'], network['b2']

# 中間層への入力u1と出力z1
u1 = np.dot(x, W1) + b1
z1 = functions.relu(u1)

# 出力層への入力u2と出力z2
u2 = np.dot(z1, W2) + b2
z2 = functions.relu(u2)
```

# Section２：活性化関数
NNにおいて、次の層への出力の大きさを決める**非線形関数**であり、入力値によって次の層への信号のON/OFFや強弱を定める働きを持つ。
```
【確認テスト】
線形と非線形の違い

線形な関数は
・加法性：f(x+y)=f(x)+f(y)
・斉次性：f(kx)=kf(x)
を満たす。

非線形な関数は加法性・斉次性を満たさない。
```
中間層と出力層でそれぞれ用いられる活性化関数がある。
* 中間層
  * ステップ関数：閾値を超えたら発火する関数で出力は常に1か0。線形分離可能なものしか学習できなかった。
  * シグモイド関数：0~1の間を緩やかに変化する関数で、信号の強弱を伝えられるようになり予想NNの普及のきっかけとなった。大きな値では出力の変化が微小なため、勾配消失問題が起きることがあった。
  * ReLU関数：ある値を境に、0か入力値を取る関数。勾配消失問題の回避とスパース化に貢献する。
* 出力層
  * ソフトマックス関数
  * 恒等写像
  * シグモイド関数

```python
# 【確認テスト】
# 活性化関数をソースコードから抜き出す。functions.relu()の部分が該当箇所

z1 = functions.relu(u1)
z2 = functions.relu(u2)
```

# Section3：出力層
出力層では中間層（入力層）から得られた値を元に、最終的な分類予想を確率として出力する。識別時には活性化関数が用いられ、学習時にはさらに誤差関数を用いる。

## 誤差関数
学習時に、予測結果と教師データの差異(誤差)を関数化したもの。
* 回帰：二乗誤差
* 分類：交差エントロピー誤差

```
【確認テスト】
二乗誤差の二乗と係数1/2の意味は？

・二乗する理由：単純な差であれば、各ラベルの誤差で正負両方の値が発生し、全体の誤差を正しく表すのに都合が悪いため、二乗してそれぞれのラベルでの誤差を正の値になるようにするため
・1/2する理由：NNの学習時の誤差逆伝播で誤差関数の微分を計算した際に計算式を簡単にするためで、本質的な意味はない
```

## 出力層の活性化関数
中間層の活性化関数との違いをまとめる

* 値の強弱：中間層では信号の強弱を調整したりするが、出力層では信号での大きさ（比率）はそのまま
* 確率を出力：出力層では、分類問題の場合は出力は0~1の範囲かつ総和が1になる

改めて中間層と出力層でそれぞれ用いられる活性化関数についてまとめる。
* 中間層
  * ステップ関数
  * シグモイド関数
  * ReLU関数
* 出力層
  * 恒等写像：回帰に用いられる
  * シグモイド関数：二値分類に用いられる
  * ソフトマックス関数：他クラス分類に用いられる

```python
# 【確認テスト】
# ソフトマックス関数をソースコードで表す

# 本質的な部分は下記
def softmax(x):
  return np.exp(x) / np.sum(np.exp(x))
  
# pythonの計算では配列とスカラーの計算ではスカラー値が配列の要素ごとに計算される
print(x)
# [1 2]
print(np.exp(x))
# [2.71828183 7.3890561 ]

# np.sumは与えた配列の要素の総和を返す
print(np.sum(np.exp(x)))
# 10.107337927389695

# i番目のソフトマックスの値を取得する場合
softmax(x)[i]
```

```python
# 【確認テスト】
# 交差エントロピーをソースコードで表す

# 本質的な部分は下記のように表させる
def cross_entropy_error(d, y):
  batch_size=y.shape[0]
  return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7))/batch_size
  
# npの計算がわかりづらかったので適当な値例で確認
y=np.array([
    [0,0.2,0.8,0,0,0,0,0,0,0],
    [0,0,0,0,0,0.2,0.6,0.2,0,0],
    [0.9,0.1,0,0,0,0,0,0,0,0],
    [0,0,0,0,0,0,0,0,0.2,0.8],
    [0,0,0,0,0.7,0.3,0,0,0,0]])
t=np.array([2,7,0,9,4])
batch_size=y.shape[0]

print(np.arange(batch_size))
# [0 1 2 3 4]
print(y[np.arange(batch_size),t])
# [0.8 0.2 0.9 0.8 0.7]
print(-np.sum(np.log(y[np.arange(batch_size),t] + 1e-7))/batch_size)
# 0.5035518941381965
```

（参考文献）ゼロから作るDeepLearning

# Section4：勾配降下法
# Section5：誤差逆伝搬法
