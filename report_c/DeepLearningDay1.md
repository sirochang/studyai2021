# プロローグ
## 識別と生成
### 識別
データを対象のクラスに分類するのが目的で、条件「あるデータが与えられた」という条件のもとで対象クラスである確率を求める。
### 生成
対象のクラスのデータを生成するのが目的で、条件「あるクラスに属する」という条件のもとでのデータの分布を求める。

## 識別モデルと生成モデル
### 識別モデル
高次元のデータから低次元のデータへ変換するモデルで、生成モデルと比べると学習に必要なデータ量が少ない。画像認識等に応用される。

### 生成モデル
低次元のデータから高次元のデータへ変換するモデルで、識別モデルと比べると学習に必要なデータ量が多い。画像の超解像やテキスト生成等に応用される。

---

# ニューラルネットワークの全体像

```
【確認テスト】
ディープラーニングとは、明示的なプログラムの代わりに多数の中間層を持つニューラルネットワーク（NN）を用いて、入力値から目的とする出力層に変換する数学モデルを構築することである。
各層の入力部分である重みとバイアスを最適化することが目的である。
```

```
【確認テスト】
次のNNを図示する。
　・入力層：2ノード1層
　・中間層：3ノード2層
　・出力層：1ノード1層
TODO
```

NNは回帰と分類のどちらの問題も解くここができる
* 回帰：連続する実数値を取る関数の近似
  * 線形回帰
  * 回帰木
  * ランダムフォレスト
  * NN
* 分類：離散的な結果を予想するための分析
  * ベイズ分類
  * ロジスティック回帰
  * 決定木
  * ランダムフォレスト
  * NN

---

# Section1：入力層〜中間層
入力とバイアスと層間の重みの線形結合を活性化関数で変換して得られた結果を結合している層の出力とする（次の層の入力となる）。

```python
# 【確認テスト】
# u = Wx + b をpythonのコードで表す。

import numpy as np

# 重み
W = np.array([[0.1], [0.2]])

# バイアス
b = 0.5

# 入力値
x = np.array([2, 3])

# 総入力
u = np.dot(x, W) + b
```

スライドNo17の図とサンプルコードが一致していないように思われます。
* 入力層として、xを活性化関数で変換してz(1)を出力としている？
* 図を見るとz(1)とW(1)とb(1)の線形結合がu(1)となっているが、ソースではu(1)を活性化関数で変換した出力をz(1)としている

以上を踏まえ、下記のように解釈しました。
```python
# 【確認テスト】
# スライドNo17のNNの中間層の出力を定義しているソースコードは？

# 入力層から中間層への重みとバイアス
W1, b1 = network['W1'], network['b1']

# 中間層から出力層への重みとバイアス
W2, b2 = network['W2'], network['b2']

# 中間層への入力u1と出力z1
u1 = np.dot(x, W1) + b1
z1 = functions.relu(u1)

# 出力層への入力u2と出力z2
u2 = np.dot(z1, W2) + b2
z2 = functions.relu(u2)
```

# Section２：活性化関数
NNにおいて、次の層への出力の大きさを決める**非線形関数**であり、入力値によって次の層への信号のON/OFFや強弱を定める働きを持つ。
```
【確認テスト】
線形と非線形の違い

線形な関数は
・加法性：f(x+y)=f(x)+f(y)
・斉次性：f(kx)=kf(x)
を満たす。

非線形な関数は加法性・斉次性を満たさない。
```
中間層と出力層でそれぞれ用いられる活性化関数がある。
* 中間層
  * ステップ関数：閾値を超えたら発火する関数で出力は常に1か0。線形分離可能なものしか学習できなかった。
  * シグモイド関数：0~1の間を緩やかに変化する関数で、信号の強弱を伝えられるようになり予想NNの普及のきっかけとなった。大きな値では出力の変化が微小なため、勾配消失問題が起きることがあった。
  * ReLU関数：ある値を境に、0か入力値を取る関数。勾配消失問題の回避とスパース化に貢献する。
* 出力層
  * ソフトマックス関数
  * 恒等写像
  * シグモイド関数

```python
# 【確認テスト】
# 活性化関数をソースコードから抜き出す。functions.relu()の部分が該当箇所

z1 = functions.relu(u1)
z2 = functions.relu(u2)
```

# Section3：出力層
出力層では中間層（入力層）から得られた値を元に、最終的な分類予想を確率として出力する。識別時には活性化関数が用いられ、学習時にはさらに誤差関数を用いる。

## 誤差関数
学習時に、予測結果と教師データの差異(誤差)を関数化したもの。
* 回帰：二乗誤差
* 分類：交差エントロピー誤差

```
【確認テスト】
二乗誤差の二乗と係数1/2の意味は？

・二乗する理由：単純な差であれば、各ラベルの誤差で正負両方の値が発生し、全体の誤差を正しく表すのに都合が悪いため、二乗してそれぞれのラベルでの誤差を正の値になるようにするため
・1/2する理由：NNの学習時の誤差逆伝播で誤差関数の微分を計算した際に計算式を簡単にするためで、本質的な意味はない
```

## 出力層の活性化関数
中間層の活性化関数との違いをまとめる

* 値の強弱：中間層では信号の強弱を調整したりするが、出力層では信号での大きさ（比率）はそのまま
* 確率を出力：出力層では、分類問題の場合は出力は0~1の範囲かつ総和が1になる

改めて中間層と出力層でそれぞれ用いられる活性化関数についてまとめる。
* 中間層
  * ステップ関数
  * シグモイド関数
  * ReLU関数
* 出力層
  * 恒等写像：回帰に用いられる
  * シグモイド関数：二値分類に用いられる
  * ソフトマックス関数：他クラス分類に用いられる

```python
# 【確認テスト】
# ソフトマックス関数をソースコードで表す

# 本質的な部分は下記
def softmax(x):
  return np.exp(x) / np.sum(np.exp(x))
  
# pythonの計算では配列とスカラーの計算ではスカラー値が配列の要素ごとに計算される
print(x)
# [1 2]
print(np.exp(x))
# [2.71828183 7.3890561 ]

# np.sumは与えた配列の要素の総和を返す
print(np.sum(np.exp(x)))
# 10.107337927389695

# i番目のソフトマックスの値を取得する場合
softmax(x)[i]
```

```python
# 【確認テスト】
# 交差エントロピーをソースコードで表す

# 本質的な部分は下記のように表させる
def cross_entropy_error(d, y):
  batch_size=y.shape[0]
  return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7))/batch_size
  
# npの計算がわかりづらかったので適当な値例で確認
y=np.array([
    [0,0.2,0.8,0,0,0,0,0,0,0],
    [0,0,0,0,0,0.2,0.6,0.2,0,0],
    [0.9,0.1,0,0,0,0,0,0,0,0],
    [0,0,0,0,0,0,0,0,0.2,0.8],
    [0,0,0,0,0.7,0.3,0,0,0,0]])
t=np.array([2,7,0,9,4])
batch_size=y.shape[0]

print(np.arange(batch_size))
# [0 1 2 3 4]
print(y[np.arange(batch_size),t])
# [0.8 0.2 0.9 0.8 0.7]
print(-np.sum(np.log(y[np.arange(batch_size),t] + 1e-7))/batch_size)
# 0.5035518941381965
```

（参考文献）ゼロから作るDeepLearning

# Section4：勾配降下法
深層学習の目的は、学習を通して誤差を最小にするネットワークを構成することで、勾配降下法を利用してパラメータを最適化する。
```python
# 【確認テスト】
# パラメータ更新式や誤差計算部分のソースコードを探す

## パラメータ更新式
network[key] -= learning_rate * grad[key]

## 誤差計算
grad = backward(x, d, z1, y)
```

学習率の値によって学習の効率が大きく異なる
* 大きすぎる場合：最小値にいつまでもたどり着かずに発散してしまう
* 小さすぎる場合：発散することはないが、収束するまでに時間がかかってしまったり、極小値を見つけると学習が進みづらくなる

誤差関数の値をより小さくする方向に重みWとバイアスbを更新し次の周（エポック）に反映する。

＊ 勾配降下法：毎回、入力データ全てを使ってパラメータを更新する
* 確率的勾配降下法：毎回、データを1つずつランダムに選んでパラメータを更新する
  * データが冗長の場合の計算コストの削減
  * 望まない局所極小解に収束するリスクの軽減
  * オンライン学習ができる
* ミニバッチ勾配降下法：毎回、入力データのいくつかを使ってパラメータを更新する
  * 確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる
  * CPUを利用したスレッド並列化やGPUを利用したSIMD並列化

```
 【確認テスト】
 オンライン学習とは何か。
 
 学習データが入ってくるたびに都度パラメータを更新し、学習を進めていく方法。
 一方、バッチ学習では一度に全ての学習データを使ってパラメータ更新を行う。
```

```
【確認テスト】
パラメータ更新式を図示する

TODO
```

誤差勾配の計算には数値微分を用いることもできるが、各パラメータWごとに誤差を計算するために順伝播の計算を繰り返し行う必要があり、負荷が大きいので誤差逆伝播法を利用する。
数値微分とは、プログラムで微小な数値を生成し擬似的に微分を計算する一般的な手法である。

# Section5：誤差逆伝播法
算出された誤差を出力層側から順に微分し、前の層へと伝播させる。最小限の計算で各パラメータでの微分値を解析的に計算できる手法である。

計算結果から微分を逆算することで、伝播前の層の微分結果を使えるので、不要な再帰的計算を避けて微分を算出できる。

```python
# 【確認テスト】
# 誤差逆伝播法で、すでに行った計算結果を保持しているソースコードを抜き出す。

# 例えば出力層の活性化関数の誤差の微分結果はdelta2として保持している。
delta2 = function.d_mean_squared_error(d, y)

# この結果を用いて、W(2)の誤差やb(2)の誤差、z（ソースコードでは中間層の活性化関数まで含めて）の誤差を計算できる
grad['W2'] = np.dot(z1.T, delta2)
grad['b2'] = np.sum(delta2, axis=0)
delta1 = np.dot(delta2, W2.T) * function.d_sigmoid(z1)
```

誤差の逆伝播で伝播前の層の微分結果を使う際に連鎖律の原理を用いる。

連鎖律とは合成関数の微分についての性質のことで、「ある関数が合成関数で表される場合、その合成関数の微分は、合成関数を構成するそれぞれの関数の微分の積によって表すことができる」ことである。

```python
# 【確認テスト】
# u(2)の誤差とW(2)の誤差のソースコードを抜き出す。

# u(2)
delta2 = function.d_mean_squared_error(d, y)

# W(2)
grad['W2'] = np.dot(z1.T, delta2)
```

今回のNNのu(2)で活性化関数は恒等写像なので、微分値は1で活性化関数前後で値は変わらない。
動画ではおそらくu(1)とW(1)のソースコードの箇所について説明されているが、資料にもある通りu(2)とW(2)は上記のソースコードとなる。
