# 概要
「機械学習」について章ごとに要点をまとめる。

# 序章
機械学習モデリングプロセスは下記の通りで、実業務であれば前半の1~3にコストがかかる。
1. 問題設定：機械学習を使う必要があるか、ルールベースを用いて解決できないかなど
2. データ選定：機械学習のモデルを作成する上で適したデータになっているか、データに偏りやバイアスがかかっていないかなど
3. データの前処理：欠損値や異常値などを除外したり修正する
4. 機械学習モデルの選定→教師あり学習と教師なし学習でそれぞれ代表的な選定方法を学習する
5. モデルの学習（パラメータの推定）→教師あり学習と教師なし学習でそれぞれ代表的な推定方法を学習する
6. モデルの評価→教師あり学習の代表的な評価方法を学習する、教師なし学習は一般的にはモデルの評価は行わない

# 第1章 線形回帰モデル
* 直線（2次元）の方程式： y = A * x + B
* 平面（3次元）の方程式： x = A * x + B * y + C
* 超平面（n次元）の方程式： y = a_0 * x_0 + a_1 * x_1 + a_2 * x_2 ... a_n-1 * x_n-1  （ただしx_0 = 1）

a_0 ~ a_n-1をまとめたベクトルを**a**、x_0 ~ x_n-1をまとめたベクトルを**x**とすると
n次元の方程式は**a**と**x**の内積をとった形で表される。

回帰問題とは「ある入力(離散あるいは連続値)から出力(連続値)を予測する問題」である。

回帰を使ってランキング予測のようなこともできるがあまりオススメはできない（**Vapnikの原理**；ある問題を解くとき、その問題よりも難しい問題を途中の段階で解いてはならない）

入力をm次元のベクトルとする（パラメータがm次元）。また、教師あり学習なのでn個のxとyのペアが与えられる。

超平面の方程式は1つの方程式だったが、線形回帰モデルではn個の連立方程式を解くことを考える。

連立方程式の解き方からわかるように、一般的に n>m+1 でないと解くことはできない（パラメータ数よりデータ数を多く用意する必要がある）
```
y(n,1;次元) = X(n,m+1;次元) * w(m+1,1;次元) + ε(n,1;次元）
```

データとモデル出力の二乗誤差の和である平均二乗誤差を最小するパラメータを算出する。（最小二乗法）
ただし最小二乗法は外れ値に影響を受けやすいなど欠点もあるので注意する。

式の導出。
行列関連の公式としてmatrix cookbookが参考になる。

## 実装
演習を実行したgoogle colaboratoryのURL：https://colab.research.google.com/drive/1GetzrGgiYbQIH7FpsLVGKfmWMXlbh8IK?usp=sharing

# 第2章 非線形回帰モデル
# 第3章 ロジスティック回帰モデル
# 第4章 主成分分析
# 第5章 アルゴリズム
# 第6章 サポートベクターマシーン
